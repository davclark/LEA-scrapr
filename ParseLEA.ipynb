{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:93bb668563cdd3703d0499a60a5b485368861468d08e8b3841d435dbb4d5ce92"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "from os import path\n",
      "from glob import glob\n",
      "\n",
      "from lxml import etree\n",
      "from lxml.cssselect import CSSSelector as CSS  # also needs cssselect installed\n",
      "from six import print_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notes\n",
      "\n",
      "Sadly, the pages use CSS or something to make the checkboxes. So, when you look at the bare file, the checked boxes look like this: \u00e2\u02dc\u2018 (unchecked look like \u00e2\u02dc). This is likely because Chrome saves files at utf-8 by default, but then renders them as iso-latin or something dumb like that. Thanks Chrome!\n",
      "\n",
      "If you look at the source in a text editor, though, you can see the checkboxes fine.\n",
      "\n",
      "Also, the HTML structure borders on malicious. There is no useful structure (likely HTML was largely driven by layout concerns)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "An example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls scraped-pages/Adelanto\\ Elementary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Melva Davis Academy of Excellence.html\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "fname = './scraped-pages/Adelanto Elementary/Melva Davis Academy of Excellence.html'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LEA_parser:\n",
      "    # I'm trying to use CSS selectors where possible, as more folks will know these\n",
      "    LEA_info_fields = ['Name of LEA', 'CDS Code', 'Type of LEA', 'County', \n",
      "                       'Contact Name', 'Contact Phone', 'Contact Email']\n",
      "    fields_xp = ['.//b[text()=\"{}:\"]'.format(field) for field in LEA_info_fields]\n",
      "    lists_loc = CSS('ol.zeroindent li')\n",
      "    parse_utf8 = etree.HTMLParser(encoding='utf-8')\n",
      "\n",
      "    @classmethod\n",
      "    def header(cls):\n",
      "        return LEA_parser.LEA_info_fields + list(range(1, 26))\n",
      "    \n",
      "    def __init__(self, filename):\n",
      "        self.parsed = etree.parse(filename, self.parse_utf8)\n",
      "        self.extract_info()\n",
      "        self.extract_responses()\n",
      "\n",
      "    def extract_info(self):\n",
      "        elts = [self.parsed.xpath(xp)[0] for xp in self.fields_xp]\n",
      "        # A little hairy... There may be text both before and after the bolded field name\n",
      "        # We want the last element, which will be what comes after.\n",
      "        self.info = [elt.getparent().xpath('text()')[-1].strip() for elt in elts]\n",
      "        \n",
      "    def extract_responses(self):\n",
      "        curr_items = self.lists_loc(self.parsed)\n",
      "    \n",
      "        self.res = []\n",
      "    \n",
      "        for item in curr_items:\n",
      "            # CSS won't let us grab stuff based on content, only structure\n",
      "            # So, we're using Xpath (and we're using unicode because of how Chrome saved these)\n",
      "            checkboxes = item.xpath('.//span[text()=\"\u2611\"]')\n",
      "            if not checkboxes:\n",
      "                # string() exports the text of children also... not sure why, \n",
      "                # but text() doesn't work here\n",
      "                if item.xpath('.//*[contains(string(), \"No response\")]'):\n",
      "                    self.res.append('No response')\n",
      "                else:\n",
      "                    self.res.append('Something is amiss with the parser:' + item.xpath('string()'))\n",
      "            else:\n",
      "                # Some sections are multiple choice!\n",
      "                checked_vals = '; '.join(self.get_box_label(box) for box in checkboxes)\n",
      "                self.res.append(checked_vals)\n",
      "\n",
      "                \n",
      "    def get_box_label(self, box_el):\n",
      "        '''Get the label to the right of the checkbox\n",
      "\n",
      "        Currently, this doesn't handle \"(please specify)\" information'''\n",
      "        return box_el.getnext().xpath('string()')\n",
      "    \n",
      "    def as_list(self):\n",
      "        return self.info + self.res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Seems to be working!\n",
      "sample_parsed = LEA_parser(fname)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_parsed.info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "['Taylion High Desert Academy/Adelanto (Adelanto)',\n",
        " '36675870128462',\n",
        " 'Direct-Funded Charter School',\n",
        " 'San Bernardino',\n",
        " 'Danielle Moore',\n",
        " '760-246-3344',\n",
        " 'danielle.moore@learningmatters.org']"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Do it!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('parsed_LEA.csv', 'w') as outfile:\n",
      "    outfile = csv.writer(outfile)\n",
      "    # range is up to, but not including the upper limit\n",
      "    outfile.writerow(['District', 'School'] + LEA_parser.header())\n",
      "\n",
      "    for resp_file in glob('scraped-pages/*/*.html'):\n",
      "        if '00-NotSubmitted' in resp_file or \\\n",
      "           '00-Glitch' in resp_file:\n",
      "            # These don't have any data in them. Just keeping the files\n",
      "            # because why delete anything?\n",
      "            continue\n",
      "\n",
      "        parsed = LEA_parser(resp_file)\n",
      "\n",
      "        # path.split is annoying - it only does one split\n",
      "        path_elts = resp_file.split('/')\n",
      "        district = path_elts[-2]\n",
      "        school, _ = path.splitext(path_elts[-1])\n",
      "        school = school.replace('_', '/')\n",
      "\n",
      "        outfile.writerow([district, school] + parsed.as_list())\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    }
   ],
   "metadata": {}
  }
 ]
}